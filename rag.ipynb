{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128 --quiet #!!!! nicht entfernen nur auskommentieren\n",
    "!pip install sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas numpy accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dilan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#------------------------------------------------------------------------------------------------\n",
    "#os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only for my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fde71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n",
    "    print(f\"Accelerator device: {accelerator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_mistral,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_mistral = accelerator.prepare(model_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_deepseek,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_deepseek = accelerator.prepare(model_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_llama,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_llama = accelerator.prepare(model_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#\"Qwen/Qwen2.5-0.5B\" -> \"Qwen/Qwen2-7B\"\n",
    "model_name_qwen = \"Qwen/Qwen2-7B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [02:36<00:00,  1.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=5):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))\n",
    "    \n",
    "    # Filter: nur Chunks mit mehr als 50 Zeichen UND Frage-Keywords enthalten\n",
    "    keywords = [w.lower() for w in question.split()]\n",
    "    selected_chunks = []\n",
    "    for i in indices[0]:\n",
    "        chunk = chunks[i].strip()\n",
    "        if len(chunk) > 50 and any(kw in chunk.lower() for kw in keywords):\n",
    "            selected_chunks.append(chunk)\n",
    "        if len(selected_chunks) >= k:\n",
    "            break\n",
    "    return \"\\n\".join(selected_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=800):\n",
    "    tokanized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokanized_input[\"input_ids\"]\n",
    "\n",
    "    prompt_length = input_ids.shape[1]\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids).logits\n",
    "        output = output.squeeze(dim=0)\n",
    "        next_token_scores = output[-1]\n",
    "        next_token_id = next_token_scores.argmax(dim=-1)\n",
    "        input_ids = torch.cat((input_ids, torch.LongTensor([next_token_id]).reshape(1, -1)), dim=-1)\n",
    "\n",
    "    \n",
    "    generated_ids = input_ids[0, prompt_length:]\n",
    "\n",
    "    return tokenizer.decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=50, k=5):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,        # deterministisch\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    answer = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Nur erster Satz behalten\n",
    "    for sep in [\".\", \"!\", \"?\"]:\n",
    "        if sep in answer:\n",
    "            answer = answer.split(sep)[0] + sep\n",
    "            break\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Split the following answer into simple factual statements.\n",
    "Return one statement per line. Do not add anything else.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [s.strip(\"- \").strip() for s in output.split(\"\\n\") if len(s.strip()) > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate up to 3 questions that could be answered by the following answer.\n",
    "Return one question per line.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Frage und Antwort direkt\n",
    "    und skaliert sie auf [0,1].\n",
    "    \"\"\"\n",
    "    if not answer or answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    emb = embed_model.encode([question, answer])\n",
    "    \n",
    "  \n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    \n",
    " \n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    " \n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": faithfulness < 1.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "For each of the statements below, decide if it is fully supported by the context.\n",
    "Respond with ONLY 'Yes' or 'No'.\n",
    "Do not add any explanations.\n",
    "\n",
    "Examples:\n",
    "Statement: \"Grasse Cathedral is the town's most notable landmark.\"\n",
    "Answer: Yes\n",
    "\n",
    "Statement: \"Grasse is the capital of France.\"\n",
    "Answer: No\n",
    "\n",
    "Now evaluate the following statements:\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2ccceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_vs_reference(question, generated_answer, reference_answer, context, model, tokenizer):\n",
    "    # 1. Answer Relevance berechnen (semantische Ähnlichkeit)\n",
    "    answer_relevance = calculate_answer_relevance_direct(reference_answer, generated_answer)\n",
    "\n",
    "    # 2. Faithfulness prüfen (bleibt wie bisher)\n",
    "    statements = extract_statements(generated_answer, model, tokenizer)[:10]\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "    # 3. Hallucination bestimmen\n",
    "    # True, wenn Antwort komplett falsch ist oder sehr weit von Referenz entfernt\n",
    "    if not generated_answer or generated_answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        hallucination = True\n",
    "    elif answer_relevance < 0.5:  # Grenze, ab wann Antwort als zu anders gilt\n",
    "        hallucination = True\n",
    "    else:\n",
    "        hallucination = False\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": hallucination\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    results = []\n",
    "    for i in range(2):  # oder len(rag_dataset_1200)\n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        reference_answer = rag_dataset_1200[i][\"answer\"]  # die gegebene korrekte Antwort\n",
    "\n",
    "        # Modell generiert eigene Antwort\n",
    "        generated_answer = answer_with_context(\n",
    "            model=model_qwen,\n",
    "            tokenizer=tokenizer_qwen,\n",
    "            question=question,\n",
    "            max_tokens=50,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        # Scores basierend auf Referenz-Antwort berechnen\n",
    "        scores = evaluate_generated_vs_reference(\n",
    "            question,\n",
    "            generated_answer,\n",
    "            reference_answer,\n",
    "            context,\n",
    "            model_qwen,\n",
    "            tokenizer_qwen\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"reference_answer\": reference_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            **scores\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      "lso wildly wondered if I was perhaps part of an experiment; \"The seemingly better dressed--and cleaner--of the two asks the customers for money (a specific amount, no less)...what will be the result? How will the customers react? What about the other man? Will he be acknowledged and rewarded? Or will he be 'punished' for being quiet and not asking? \"No food for you 'cause you didn't ask--just as well, saved me a buck...\"\n",
      "Was the one who approached me even homeless? Or just testing society?\n",
      "Inter\n",
      "of their inner self. Often times, we are creatures quick to judge and we start making our own conclusions about them; but instead, remember that these people have their struggles too and show some kindness because you don’t know who might need it. Like the quote says “Don’t judge people, you never know what kind of battles they are fighting”.\n",
      "Duplicate your love\n",
      "We all have a little love to give someone and showing a little TLC can never hurt anybody. Instead of keeping all the love to yourself\n",
      "s are who they say they are.\n",
      "[image via screengrab]\n",
      "t they are seeking or requiring. It waste materials their time and make sure they are furious. Be sure your site only pops up to individuals who are seeking it through the use of appropriate tags, not kinds that you just think can get you increased searching effects.\n",
      "To know in case your seo is functioning or perhaps not, it's important to monitor your pursuit search positions. Use instruments from different search engines to observe your page ranking. Take a look at site's referrer sign consist\n",
      "ng people to turn around from their self-destructive ways.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer by LLM:\n",
      "The person seeking is seeking or requiring.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores:\n",
      "{'faithfulness': 0.2, 'answer_relevance': np.float32(0.7127337), 'hallucination': False}\n"
     ]
    }
   ],
   "source": [
    "# Kontext über deinen Retriever abrufen\n",
    "context = retrieve_context(question, k=5)\n",
    "\n",
    "# Abgerufenen Kontext anzeigen\n",
    "print(\"Retrieved Context:\")\n",
    "print(context)\n",
    "print(\"\\n---\\n\")\n",
    "question=\"What is the person seeking in the context?\"\n",
    "# Die gegebene, korrekte Antwort (Referenz)\n",
    "reference_answer = \"The person is seeking a woman who wants to watch him on cam, and a real lady in his life, not a little girl.\"\n",
    "\n",
    "# Prompt wird intern in answer_with_context erstellt\n",
    "generated_answer = answer_with_context(\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    question=question,\n",
    "    max_tokens=50,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"Generated Answer by LLM:\")\n",
    "print(generated_answer)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "# Scores basierend auf der Referenz-Antwort berechnen\n",
    "scores = evaluate_generated_vs_reference(\n",
    "    question=question,\n",
    "    generated_answer=generated_answer,\n",
    "    reference_answer=reference_answer,\n",
    "    context=context,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen\n",
    ")\n",
    "\n",
    "print(\"Evaluation Scores:\")\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
