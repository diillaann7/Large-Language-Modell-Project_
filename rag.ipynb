{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "This project analyzes how different Large Language Models (LLMs) respond to Retrieval-Augmented Generation (RAG). We based our implementation on the paper \"RAGAS: Automated Evaluation of Retrieval Augmented Generation\" by Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. To evaluate the RAG performance of the different models, we implemented the three different scores mentioned in the paper: faithfulness, answer relevance, and context relevance. For instructions on how to run the project see the README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cu128 --quiet \n",
    "!pip install ipywidgets sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas accelerate numpy hf_xet tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "#------------------------------------------------------------------------------------------------\n",
    "os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only for my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5181558",
   "metadata": {},
   "source": [
    "Checks if CUDA is available and sets the accelerator for hardware-agnostic execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fde71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU Name: NVIDIA GeForce RTX 5060 Ti\n",
      "Anzahl GPUs: 1\n",
      "CUDA Version (PyTorch): 12.8\n",
      "Accelerator device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n",
    "    print(f\"Accelerator device: {accelerator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7a0ed",
   "metadata": {},
   "source": [
    "Due to hardware limitations, we were unable to run all models directly one afer the other, so we always hat to comment out all models except the one we were currently using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b3d4ed00f345a7ae618c164e6ce989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_mistral,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_mistral = accelerator.prepare(model_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_deepseek,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_deepseek = accelerator.prepare(model_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_llama,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_llama = accelerator.prepare(model_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#-> \"Qwen/Qwen3-8B\"\n",
    "model_name_qwen = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"all\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10950"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f6340f5df04b87b86ec1c42dc837da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10950, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1206e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnerate_answer(model, tokenizer, prompt, context=None, max_tokens=1000, do_sample=True):\n",
    "    if context==None:\n",
    "        rag_instruction = prompt\n",
    "    else:\n",
    "        rag_instruction = (\n",
    "            \"Answer using the information from the given context.\\n\"\n",
    "            f\"context: {context}\\n\\n\"\n",
    "            f\"{prompt}\"\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": rag_instruction}\n",
    "    ]\n",
    "    \n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        #-----------------------------------------------------------\n",
    "        enable_thinking=False #-> falls kein thinking model löschen!\n",
    "        #-----------------------------------------------------------\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8aa37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_statement_prompt(question: str, answer: str):\n",
    "    return f\"\"\"Given a question and answer, create one or more statements from each sentence in the given answer.\n",
    "question: {question}\n",
    "answer: {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(question: str, answer: str, model, tokenizer, max_tokens=1000):\n",
    "    #Extracts statements out of the model generated answer \n",
    "    prompt = build_statement_prompt(question, answer)\n",
    "\n",
    "    answer = gnerate_answer(model, tokenizer, prompt, max_tokens, do_sample=False)\n",
    "\n",
    "    statements = [line.strip() for line in answer.split(\"\\n\") if len(line.strip()) > 3]\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71c14b",
   "metadata": {},
   "source": [
    "Changed the prompt for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements):\n",
    "    prompt = f\"\"\"Consider the given context and following statements, the determine whether they are supported by the information presente in the context. \n",
    "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). \n",
    "Answer in the format:\n",
    "Statement: ...\n",
    "Explanation: ...\n",
    "Verdict: (Yes/No)\n",
    "Do not deviate from the specified format.\n",
    "These are the statements\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f\"Statement: {s}\\n\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66adf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_supported(answer: str):\n",
    "    yes_matches = re.findall(r'Verdict:\\s*(Yes)', answer, re.IGNORECASE)\n",
    "    yes_count = len(yes_matches)\n",
    "    return yes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091f8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_faithfulness_score(total_statements: int, suported_statements: int):\n",
    "    if suported_statements == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    return suported_statements / total_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dfa0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_answer_relevance_prompt(answer: str):\n",
    "    prompt = f\"\"\"Generate a question for the given answer.\n",
    "    answer: {answer}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(text: str) -> list:\n",
    "    text.strip()\n",
    "    list = text.split(\"?\")\n",
    "    return [item + \"?\" for item in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_question_similarity(original_question, generated_questions):\n",
    "    q_embedding = embed_model.encode([original_question])\n",
    "    gen_q_embeddings = embed_model.encode(generated_questions)\n",
    "\n",
    "    q_embedding = np.array(q_embedding).astype('float32')\n",
    "    gen_q_embeddings = np.array(gen_q_embeddings).astype('float32')\n",
    "\n",
    "    faiss.normalize_L2(q_embedding)\n",
    "    faiss.normalize_L2(gen_q_embeddings)\n",
    "\n",
    "    d = q_embedding.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    \n",
    "    index.add(gen_q_embeddings)\n",
    "    \n",
    "    k = len(generated_questions)\n",
    "    D, I = index.search(q_embedding, k=k)\n",
    "    \n",
    "    scores = D[0]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def calculate_answer_relevance_score(similatity: list):\n",
    "    return float(np.mean(similatity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ec3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_relevance_prompt(question: str):\n",
    "    prompt = f\"\"\"Please extract relevant sentences from the provided context that can potentially help ansewr the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phares \"Insufficient Information\".While extracting candidate sentences you're not allowed to make any changes to senctences from the given context.\n",
    "    Question: {question}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ae1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO vielleicht erweitern sehr simple satz zählung -> entscheiden ob auch ! und ? zählen\n",
    "def count_sentences(sentence: str):\n",
    "    num = sentence.count(\". \")\n",
    "    num += sentence.count(\"! \")\n",
    "    num += sentence.count(\"? \")\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70cf772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_context_relevance_score(num_extracted, num_sentences):\n",
    "    if num_sentences == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return num_extracted / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ba9fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_faithfullness(model, tokenizer, context, question, answer, do_sample=True):\n",
    "    #extracts statements out of the answer and generates a statements list\n",
    "    statements = extract_statements(question, answer, model_qwen, tokenizer_qwen)\n",
    "\n",
    "    #lets the model evaluate the faithfullness\n",
    "    faithfullness_prompt = build_faithfullness_prompt(statements)\n",
    "    #do_sample has to be false\n",
    "    eval = gnerate_answer(model_qwen, tokenizer_qwen, faithfullness_prompt, context, do_sample=False)\n",
    "    num_supported = count_supported(eval)\n",
    "\n",
    "    return [statements, num_supported]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ee6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO entscheiden wieviele fragen die funkition bearbeiten soll\n",
    "def analyse_answer_relevance(model, tokenizer, question, answer, do_sample=True):\n",
    "    prompt = build_answer_relevance_prompt(answer)\n",
    "    questions_generated = gnerate_answer(model, tokenizer, prompt, do_sample=do_sample)\n",
    "    questions_generated = extract_questions(questions_generated)\n",
    "\n",
    "    sim = calculate_question_similarity(question, questions_generated)\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a775af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_context_relevance(model, tokenizer, context, question):\n",
    "    prompt = build_context_relevance_prompt(question)\n",
    "    answer = gnerate_answer(model, tokenizer, prompt, context, do_sample=False)\n",
    "\n",
    "    num_extracted = count_sentences(answer)\n",
    "    num_sentences = count_sentences(context)\n",
    "    \n",
    "    return [num_extracted, num_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc4c89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_rag(model, tokenizer, do_sample=True):\n",
    "    scores = {\n",
    "        \"faithfullness score\" : 0.0,\n",
    "        \"answer relevance score\" : 0.0,\n",
    "        \"context relevance score\" : 0.0\n",
    "    }\n",
    "    #for faithfullness\n",
    "    statements = []\n",
    "    supported_statements = 0\n",
    "\n",
    "    #for answer relevance\n",
    "    similatity = []\n",
    "\n",
    "    #for context relevance\n",
    "    num_extracted = 0\n",
    "    num_sentences = 0\n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "        #generates a answer with the context for the current question\n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        answer = gnerate_answer(model_qwen, tokenizer_qwen, question, context,do_sample=do_sample)\n",
    "\n",
    "        faithfullness = analyse_faithfullness(model, tokenizer, context, question, answer, do_sample)\n",
    "        statements.extend(faithfullness[0])\n",
    "        supported_statements += faithfullness[1]\n",
    "\n",
    "        answer_relevance = analyse_answer_relevance(model, tokenizer, question, answer, do_sample=do_sample)\n",
    "        similatity.extend(answer_relevance)\n",
    "\n",
    "        context_relevance = analyse_context_relevance(model, tokenizer, context, question)\n",
    "        num_extracted += context_relevance[0]\n",
    "        num_sentences += context_relevance[1]\n",
    "\n",
    "    scores[\"faithfullness score\"] = calculate_faithfulness_score(len(statements), supported_statements)\n",
    "    scores[\"answer relevance score\"] = calculate_answer_relevance_score(similatity)\n",
    "    scores[\"context relevance score\"] = calculate_context_relevance_score(num_extracted, num_sentences)\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774d656",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d18f9f",
   "metadata": {},
   "source": [
    "Calculate scores for Qwen without temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c6aca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fa4e2694e14d6a89afbbe994da7b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyse_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_qwen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_qwen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36manalyse_rag\u001b[39m\u001b[34m(model, tokenizer, do_sample)\u001b[39m\n\u001b[32m     25\u001b[39m statements.extend(faithfullness[\u001b[32m0\u001b[39m])\n\u001b[32m     26\u001b[39m supported_statements += faithfullness[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m answer_relevance = \u001b[43manalyse_answer_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m similatity.extend(answer_relevance)\n\u001b[32m     31\u001b[39m context_relevance = analyse_context_relevance(model, tokenizer, context, question)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36manalyse_answer_relevance\u001b[39m\u001b[34m(model, tokenizer, question, answer, do_sample)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyse_answer_relevance\u001b[39m(model, tokenizer, question, answer, do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      3\u001b[39m     prompt = build_answer_relevance_prompt(answer)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     questions_generated = \u001b[43mgnerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     questions_generated = extract_questions(questions_generated)\n\u001b[32m      7\u001b[39m     sim = calculate_question_similarity(question, questions_generated)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mgnerate_answer\u001b[39m\u001b[34m(model, tokenizer, prompt, context, max_tokens, do_sample)\u001b[39m\n\u001b[32m     24\u001b[39m inputs = tokenizer(text_input, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(accelerator.device)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m generated_ids = output_ids[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:]\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\karl\\Large-Language-Modell-Project_\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\karl\\Large-Language-Modell-Project_\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2566\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2563\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2565\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2566\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2578\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2579\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2580\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2581\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\karl\\Large-Language-Modell-Project_\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2783\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2779\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2781\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   2782\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2783\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m   2786\u001b[39m         outputs = \u001b[38;5;28mself\u001b[39m(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\karl\\Large-Language-Modell-Project_\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:605\u001b[39m, in \u001b[36mGenerationMixin.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    600\u001b[39m     attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    601\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(position_ids_key) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    602\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m position_ids_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(inspect.signature(\u001b[38;5;28mself\u001b[39m.forward).parameters.keys())\n\u001b[32m    603\u001b[39m ):\n\u001b[32m    604\u001b[39m     position_ids = attention_mask.long().cumsum(-\u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[43mposition_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     kwargs[position_ids_key] = position_ids  \u001b[38;5;66;03m# placed in kwargs for further processing (see below)\u001b[39;00m\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# 5. Slice model inputs if it's an input that should have the same length as `input_ids`\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "analyse_rag(model_qwen, tokenizer_qwen, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf195a",
   "metadata": {},
   "source": [
    "Calculate scores for Mystral with temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_rag(model_deepseek, tokenizer_deepseek,do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b48cdd",
   "metadata": {},
   "source": [
    "Calculate scores for Mystral with temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e959b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_rag(model_deepseek, tokenizer_deepseek,do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1116353",
   "metadata": {},
   "source": [
    "Calculate scores for DeepSeek with temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27011c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_rag(model_deepseek, tokenizer_deepseek,do_sample=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
