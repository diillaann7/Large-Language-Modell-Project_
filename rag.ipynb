{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128 --quiet\n",
    "!pip install sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas accelerate numpy hf_xet --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#------------------------------------------------------------------------------------------------\n",
    "os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only fpr my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a378fa",
   "metadata": {},
   "source": [
    "Test if CUDA is available and set the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU Name: NVIDIA GeForce RTX 5060 Ti\n",
      "Anzahl GPUs: 1\n",
      "CUDA Version (PyTorch): 12.8\n",
      "Accelerator device: cuda\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to: D:/AI_Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(model_name_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#\"Qwen/Qwen2.5-0.5B\" -> \"Qwen/Qwen2-7B\"\n",
    "model_name_qwen = \"Qwen/Qwen2-7B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 960/960 [00:00<00:00, 48082.59 examples/s]\n",
      "Generating test split: 100%|██████████| 240/240 [00:00<00:00, 42372.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [00:08<00:00, 32.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=accelerator.device)\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=10):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))  \n",
    "    selected_chunks = [chunks[i] for i in indices[0] if len(chunks[i].strip()) > 50]\n",
    "    return \"\\n\".join(selected_chunks[:k])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "Return ONLY complete sentences.\n",
    "Do NOT include explanations, commentary, or unrelated text.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=200):\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\") -> falls keine CUDA fähige grafikkarte verwendet wird entkommentieren\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #-> evtl do_sample auf false\n",
    "            temperature=0.01,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f9f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_numbers(a, b):\n",
      "    return a + b\n"
     ]
    }
   ],
   "source": [
    "a = answer_without_context(model_qwen, tokenizer_qwen, \"Code a function in Python that adds two numbers together!\", max_tokens=1000)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=400, k=10):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\") -> falls keine CUDA fähige grafikkarte verwendet wird entkommentieren\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #-> evtl do_sample auf false\n",
    "            temperature=0.01,    \n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(answer: str, model, tokenizer, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Splits a model-generated answer into complete factual statements.\n",
    "    Each statement will be one line. Partial words or fragments are avoided.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Split the following answer into complete factual statements.\n",
    "Return one complete sentence per line.\n",
    "Do NOT add explanations, bullet points, or partial words.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    statements = [line.strip() for line in output_text.split(\"\\n\") if len(line.strip()) > 3]\n",
    "\n",
    "    return statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate up to 3 questions that could be answered by the following answer.\n",
    "Return one question per line.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Frage und Antwort direkt\n",
    "    und skaliert sie auf [0,1].\n",
    "    \"\"\"\n",
    "    if not answer or answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    emb = embed_model.encode([question, answer])\n",
    "    \n",
    "  \n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    \n",
    " \n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    " \n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": faithfulness < 1.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "For each of the statements below, decide if it is fully supported by the context.\n",
    "Respond with ONLY 'Yes' or 'No'.\n",
    "Do not add any explanations.\n",
    "\n",
    "Examples:\n",
    "Statement: \"Grasse Cathedral is the town's most notable landmark.\"\n",
    "Answer: Yes\n",
    "\n",
    "Statement: \"Grasse is the capital of France.\"\n",
    "Answer: No\n",
    "\n",
    "Now evaluate the following statements:\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    results = []\n",
    "    for i in range(2):  \n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        answer = answer_with_context(model_qwen, tokenizer_qwen, question, max_tokens=50, k=5)\n",
    "\n",
    "        scores = evaluate_rag_answer(\n",
    "            question, answer, context, model_qwen, tokenizer_qwen\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            **scores\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVED CONTEXT:\n",
      "d on all of them (and by my count, scored on three that weren’t really chances at all!)\n",
      "- Make no mistake, these were two world-class tire fires going at it, and the Oilers have a lot to clean up, just as they did after the Flames victories. But the Sabres were by far the larger and stinkier of the two tire fires. Rewatch the game if you don’t believe me (I’m doing that right now to confirm my post game comments, and yes, it hurts a lot).\n",
      "- The truth is, we see this kind of thing regularly – goa\n",
      "Game Notes EDM vs BUF\n",
      "Buffalo Sabres (Head Coach: Dan Bylsma) at Edmonton Oilers (Head Coach: Todd McLellan)\n",
      "NHL Game #31, Rogers Place, 2016-10-16 05:00:00PM (GMT -0600)\n",
      "Sabres 6-2\n",
      "Referees: Tim Peel, Brad Watson\n",
      "Linesmen: Ryan Gibbons, Devin Berg\n",
      "Three Stars: Ryan O’Reilly; Brian Gionta; Leon Draisaitl\n",
      "- OK, look, I know we’re all disgusted at a big loss to a badly injured bottom feeder like Buffalo. But it’s important not to get overly caught up in the post facto narrative your brain is going\n",
      "falo. But it’s important not to get overly caught up in the post facto narrative your brain is going to create as it looks through the lens of that putrid score.\n",
      "- The truth is that the Oilers had a sh*t ton of great chances, and whiffed on a bunch of them. And the few they didn’t whiff on were mostly stopped.\n",
      "- The Sabres on the other hand had fewer and less dangerous chances, but seemingly scored on all of them (and by my count, scored on three that weren’t really chances at all!)\n",
      "- Make no mi\n",
      " TOI\n",
      "(click any image to embiggen)\n",
      "NHL Media Highlights\n",
      "Click the link to play the associated highlight video\n",
      "Okposo’s first goal with Sabres\n",
      "Lucic’s first goal as an Oiler\n",
      "Gionta’s tip-in goal\n",
      "Lehner’s point-blank save\n",
      "Gionta’s second goal of game\n",
      "Pouliot buries the rebound\n",
      "O’Reilly’s power-play goal\n",
      "Talbot’s great glove save\n",
      "Lehner’s strong kick save\n",
      "O’Reilly’s goal from center ice\n",
      "Moulson’s top-shelf goal\n",
      "Advertisements\n",
      "4 thoughts on “G3 2016-10-16 Ermagerd, wournded Burffalo r darngerous!”\n",
      "I\n",
      "? “Goaltending is 50% of the game – unless you don’t have it, in which case it’s 100%”. Yup.\n",
      "- On a side note, the reffing was horrific, as it always is when Tim Peel is soiling the ice with his presence. But that’s not why the Oilers lost.\n",
      "Game at a Glance\n",
      "Leaderboard\n",
      "- Milan Lucic, Benoit Pouliot each had 1 goal\n",
      "- Leon Draisaitl blasted 6 shots on goal\n",
      "- Leon Draisaitl was most active on the team with 8 shot attempts\n",
      "- Leon Draisaitl was the faceoff champ at 70.0% (7-3)\n",
      "- Patrick Maroon, Jesse\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "The three stars in the NHL game between Buffalo Sabres and Edmonton Oilers were Ryan O’Reilly, Brian Gionta, and Leon Draisaitl.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2534: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m scores = \u001b[43mevaluate_rag_answer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m=\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_qwen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_qwen\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEVALUATION SCORES:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mevaluate_rag_answer\u001b[39m\u001b[34m(question, answer, context, model, tokenizer)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mi don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manswer_relevance\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhallucination\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m statements = \u001b[43mextract_statements\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m[:\u001b[32m10\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m statements:\n\u001b[32m     16\u001b[39m     faithfulness = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mextract_statements\u001b[39m\u001b[34m(answer, model, tokenizer, max_tokens)\u001b[39m\n\u001b[32m     17\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m output_text = tokenizer.decode(output_ids[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m statements = [line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m output_text.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line.strip()) > \u001b[32m3\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2566\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2563\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2565\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2566\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2578\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2579\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2580\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2581\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2786\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2783\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2786\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2787\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2788\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:345\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou must specify exactly one of input_ids or inputs_embeds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     past_key_values = DynamicCache(config=\u001b[38;5;28mself\u001b[39m.config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:189\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juliu\\Documents\\karl_info\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2567\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2561\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2564\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2565\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2566\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "question = \"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"\n",
    "\n",
    "\n",
    "context = retrieve_context(question, k=5)\n",
    "\n",
    "print(\"RETRIEVED CONTEXT:\")\n",
    "print(context)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "answer = answer_with_context(\n",
    "    model=model_qwen, \n",
    "    tokenizer=tokenizer_qwen, \n",
    "    question=question,  \n",
    "    max_tokens=200,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "scores = evaluate_rag_answer(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    context=context,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen\n",
    ")\n",
    "\n",
    "print(\"EVALUATION SCORES:\")\n",
    "print(scores)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
