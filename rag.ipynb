{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128 --quiet \n",
    "!pip install ipywidgets sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas accelerate numpy hf_xet tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#------------------------------------------------------------------------------------------------\n",
    "#os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only for my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fde71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU Name: NVIDIA GeForce RTX 2060\n",
      "Anzahl GPUs: 1\n",
      "CUDA Version (PyTorch): 12.8\n",
      "Accelerator device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n",
    "    print(f\"Accelerator device: {accelerator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1bd52162fe45ebac998e055895cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_mistral,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_mistral = accelerator.prepare(model_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_deepseek,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_deepseek = accelerator.prepare(model_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_llama,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_llama = accelerator.prepare(model_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#\"Qwen/Qwen2.5-0.5B\" -> \"Qwen/Qwen2-7B\"\n",
    "model_name_qwen = \"Qwen/Qwen3-1.7B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7913f199024fc5bae190cb0a105d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def retrieve_context(question, k=5):\\n    q_emb = embed_model.encode([question])\\n    distances, indices = index.search(q_emb, len(chunks))\\n\\n    # Filter: nur Chunks mit mehr als 50 Zeichen UND Frage-Keywords enthalten\\n    keywords = [w.lower() for w in question.split()]\\n    selected_chunks = []\\n    for i in indices[0]:\\n        chunk = chunks[i].strip()\\n        if len(chunk) > 50 and any(kw in chunk.lower() for kw in keywords):\\n            selected_chunks.append(chunk)\\n        if len(selected_chunks) >= k:\\n            break\\n    return \"\\n\".join(selected_chunks)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def retrieve_context(question, k=5):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))\n",
    "    \n",
    "    # Filter: nur Chunks mit mehr als 50 Zeichen UND Frage-Keywords enthalten\n",
    "    keywords = [w.lower() for w in question.split()]\n",
    "    selected_chunks = []\n",
    "    for i in indices[0]:\n",
    "        chunk = chunks[i].strip()\n",
    "        if len(chunk) > 50 and any(kw in chunk.lower() for kw in keywords):\n",
    "            selected_chunks.append(chunk)\n",
    "        if len(selected_chunks) >= k:\n",
    "            break\n",
    "    return \"\\n\".join(selected_chunks)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"Answer the question using ONLY the information from the context.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO entscheide bei jedem max_tokens=1000 ob 1000 angebracht sind\n",
    "def answer_without_context(model, tokenizer, question, max_tokens=1000, do_sample=True):\n",
    "    #TODO entscheiden ob der system prompt bleibt für die RAG analyse\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        #-----------------------------------------------------------\n",
    "        enable_thinking=False #-> falls kein thinking model löschen!\n",
    "        #-----------------------------------------------------------\n",
    "    )\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1206e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_with_context(model, tokenizer, question, context, max_tokens=1000, do_sample=True):\n",
    "    rag_instruction = (\n",
    "        \"Answer the question using the information from the given context.\\n\"\n",
    "        f\"question: {question}\\n\"\n",
    "        f\"context: {context}\"\n",
    "    )\n",
    "\n",
    "    #TODO entscheiden ob der system prompt bleibt für die RAG analyse\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": rag_instruction}\n",
    "    ]\n",
    "    \n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        #-----------------------------------------------------------\n",
    "        enable_thinking=False #-> falls kein thinking model löschen!\n",
    "        #-----------------------------------------------------------\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _answer_with_context(model, tokenizer, question, max_tokens=50, k=5):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,        # deterministisch\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    answer = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Nur erster Satz behalten\n",
    "    for sep in [\".\", \"!\", \"?\"]:\n",
    "        if sep in answer:\n",
    "            answer = answer.split(sep)[0] + sep\n",
    "            break\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8aa37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_statement_prompt(question: str, answer: str):\n",
    "    return f\"\"\"Given a question and answer, create one or more statements from each sentence in the given answer.\n",
    "question: {question}\n",
    "answer: {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(question: str, answer: str, model, tokenizer, max_tokens=1000):\n",
    "    #Extracts statements out of the model generated answer \n",
    "    prompt = build_statement_prompt(question, answer)\n",
    "\n",
    "    answer = answer_without_context(model, tokenizer, prompt, max_tokens, do_sample=False)\n",
    "\n",
    "    statements = [line.strip() for line in answer.split(\"\\n\") if len(line.strip()) > 3]\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71c14b",
   "metadata": {},
   "source": [
    "Changed the prompt for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements):\n",
    "    prompt = f\"\"\"Consider the given context and following statements, the determine whether they are supported by the information presente in the context. \n",
    "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). \n",
    "Answer in the format:\n",
    "Statement: ...\n",
    "Explanation: ...\n",
    "Verdict: (Yes/No)\n",
    "Do not deviate from the specified format.\n",
    "These are the statements\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f\"Statement: {s}\\n\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e66adf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO zählt aktuell die anzahl von \"Yes\" soll aber richtig die anzahl der supported statements zählen -> vlt zu get_statements()\n",
    "def count_supported(prompt: str):\n",
    "    return prompt.count(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091f8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_faithfulness_score(total_statements: int, suported_statements: int):\n",
    "    if suported_statements == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    return suported_statements / total_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dfa0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_answer_relevance_prompt(answer: str):\n",
    "    prompt = f\"\"\"Generate a question for the given answer.\n",
    "    answer: {answer}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def calculate_similarity(q: str, q_answer: str) -> float:\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_relevance_prompt(question: str):\n",
    "    prompt = f\"\"\"Please extract relevant sentences from the provided context that can potentially help ansewr the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phares \"Insufficient Information\".While extracting candidate sentences you're not allowed to make any changes to senctences from the given context.\n",
    "    Question: {question}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5ae1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO vielleicht erweitern sehr simple satz zählung\n",
    "def count_sentences(sentence: str):\n",
    "    return sentence.count(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70cf772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_context_relevance_score(num_extracted, num_sentences):\n",
    "    if num_sentences == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return num_extracted / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ba9fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_faithfullness(model, tokenizer, context, question, answer, do_sample=True):\n",
    "    #extracts statements out of the answer and generates a statements list\n",
    "    statements = extract_statements(question, answer, model_qwen, tokenizer_qwen)\n",
    "\n",
    "    #lets the model evaluate the faithfullness\n",
    "    faithfullness_prompt = build_faithfullness_prompt(statements)\n",
    "    #do_sample has to be false\n",
    "    eval = answer_with_context(model_qwen, tokenizer_qwen, context, faithfullness_prompt, do_sample=False)\n",
    "    num_supported = count_supported(eval)\n",
    "\n",
    "    return [statements, num_supported]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO entscheiden wieviele fragen die funkition bearbeiten soll\n",
    "def analyse_answer_relevance(model, tokenizer, answer, do_sample=True):\n",
    "    prompt = build_answer_relevance_prompt(answer)\n",
    "    llm_question = answer_without_context(model, tokenizer, prompt, do_sample=do_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a775af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_context_relevance(model, tokenizer, context, question):\n",
    "    prompt = build_context_relevance_prompt(question)\n",
    "    print(prompt)\n",
    "    answer = answer_with_context(model, tokenizer, context, prompt, do_sample=False)\n",
    "\n",
    "    num_extracted = count_sentences(answer)\n",
    "    num_sentences = count_sentences(context)\n",
    "    \n",
    "    return [num_extracted, num_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc4c89ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ae222beeae4216b2732a09aa9200d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statement: 1. The new website for the Amsterdamse Kunstraad was designed to offer users a clear and functional overview of all the content.  \n",
      "Explanation: The context states that the main goal was to give the user a clear and functional overview of all the content.  \n",
      "Verdict: Yes  \n",
      "\n",
      "Statement: 2. The primary purpose of the new website was to provide a comprehensive and organized view of the content available on the Amsterdamse Kunstraad.  \n",
      "Explanation: The context mentions that the goal was to provide a clear and functional overview of all the content, which implies a comprehensive and organized view.  \n",
      "Verdict: Yes\n",
      "Please extract relevant sentences from the provided context that can potentially help ansewr the following question.If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phares \"Insufficient Information\".While extracting candidate sentences you're not allowed to make any changes to senctences from the given context.\n",
      "    Question: What was the main goal of the new website for the Amsterdamse Kunstraad?\n",
      "{'faithfullness score': 1.0, 'answer relevance score': 0, 'context relevance score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def analyse_rag(model, tokenizer, do_sample=True):\n",
    "    scores = {\n",
    "        \"faithfullness score\" : 0,\n",
    "        \"answer relevance score\" : 0,\n",
    "        \"context relevance score\" : 0\n",
    "    }\n",
    "    #for faithfullness\n",
    "    statements = []\n",
    "    supported_statements = 0\n",
    "\n",
    "    #for answer relevance\n",
    "\n",
    "    #for context relevance\n",
    "    num_extracted = 0\n",
    "    num_sentences = 0\n",
    "\n",
    "    for i in tqdm(range(1)):\n",
    "        #generates a answer with the context for the current question\n",
    "        question = rag_dataset_1200[90+i][\"question\"]\n",
    "        context = rag_dataset_1200[90+i][\"context\"]\n",
    "        answer = answer_with_context(model_qwen, tokenizer_qwen, context, question, do_sample=do_sample)\n",
    "\n",
    "        faithfullness = analyse_faithfullness(model, tokenizer, context, question, answer, do_sample)\n",
    "        statements.extend(faithfullness[0])\n",
    "        supported_statements += faithfullness[1]\n",
    "\n",
    "\n",
    "        #TODO\n",
    "        #answer_relevance = analyse_answer_relevance()\n",
    "\n",
    "        context_relevance = analyse_context_relevance(model, tokenizer, context, question)\n",
    "        num_extracted += context_relevance[0]\n",
    "        num_sentences += context_relevance[1]\n",
    "\n",
    "    scores[\"faithfullness score\"] = calculate_faithfulness_score(len(statements), supported_statements)\n",
    "    scores[\"context relevance score\"] = calculate_context_relevance_score(num_extracted, num_sentences)\n",
    "\n",
    "    return scores\n",
    "\n",
    "print(analyse_rag(model_qwen, tokenizer_qwen, do_sample=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
