{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "This project analyzes how different Large Language Models (LLMs) respond to Retrieval-Augmented Generation (RAG). We based our implementation on the paper \"RAGAS: Automated Evaluation of Retrieval Augmented Generation\" by Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. To evaluate the RAG performance of the different models, we implemented the three different scores mentioned in the paper: faithfulness, answer relevance, and context relevance. For instructions on how to run the project see the README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cu128 --quiet\n",
    "!pip install -U bitsandbytes --quiet \n",
    "!pip install ipywidgets sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas accelerate numpy hf_xet tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "#------------------------------------------------------------------------------------------------\n",
    "#os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only for my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5181558",
   "metadata": {},
   "source": [
    "Checks if CUDA is available and sets the accelerator for hardware-agnostic execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fde71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: False\n",
      "Accelerator device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n",
    "    print(f\"Accelerator device: {accelerator.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7a0ed",
   "metadata": {},
   "source": [
    "Due to hardware limitations, we were unable to run all models directly one afer the other, so we always hat to comment out all models except the one we were currently using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>=0.46.1`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     28\u001b[39m model_name_qwen = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-0.6B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     31\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     32\u001b[39m     bnb_4bit_compute_dtype=torch.float16, \u001b[38;5;66;03m# Use half-precision for speed\u001b[39;00m\n\u001b[32m     33\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     34\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m model_qwen = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_qwen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- Add this\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- Vital for RTX cards!\u001b[39;49;00m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m model_qwen = accelerator.prepare(model_qwen) \n\u001b[32m     44\u001b[39m tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/WS25-26/Programming for Modern Machine Learning/project/Large-Language-Modell-Project_/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/WS25-26/Programming for Modern Machine Learning/project/Large-Language-Modell-Project_/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4015\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mexperts_implementation\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   4013\u001b[39m     config._experts_implementation = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mexperts_implementation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4015\u001b[39m hf_quantizer, config, device_map = \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[32m   4017\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file:\n\u001b[32m   4020\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/WS25-26/Programming for Modern Machine Learning/project/Large-Language-Modell-Project_/.venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:326\u001b[39m, in \u001b[36mget_hf_quantizer\u001b[39m\u001b[34m(config, quantization_config, device_map, weights_only, user_agent)\u001b[39m\n\u001b[32m    323\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n\u001b[32m    331\u001b[39m     config = hf_quantizer.update_tp_plan(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Studium/WS25-26/Programming for Modern Machine Learning/project/Large-Language-Modell-Project_/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:60\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     57\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     61\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBITSANDBYTES_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     66\u001b[39m validate_bnb_backend_availability(raise_exception=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>=0.46.1`"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_mistral,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_mistral = accelerator.prepare(model_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_deepseek,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_deepseek = accelerator.prepare(model_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_llama,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_llama = accelerator.prepare(model_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#-> \"Qwen/Qwen3-8B\"\n",
    "model_name_qwen = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use half-precision for speed\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)\n",
    "tokenizer_qwen.padding_side = \"left\"\n",
    "if tokenizer_qwen.pad_token is None:\n",
    "    tokenizer_qwen.pad_token = tokenizer_qwen.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"all\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10950"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a01f99dc854c12b23c9c8e618db17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cdb85632a34dbd8d18a3182395e2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10950, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def retrieve_context(question, k=5):\\n    q_emb = embed_model.encode([question])\\n    distances, indices = index.search(q_emb, len(chunks))\\n\\n    # Filter: nur Chunks mit mehr als 50 Zeichen UND Frage-Keywords enthalten\\n    keywords = [w.lower() for w in question.split()]\\n    selected_chunks = []\\n    for i in indices[0]:\\n        chunk = chunks[i].strip()\\n        if len(chunk) > 50 and any(kw in chunk.lower() for kw in keywords):\\n            selected_chunks.append(chunk)\\n        if len(selected_chunks) >= k:\\n            break\\n    return \"\\n\".join(selected_chunks)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def retrieve_context(question, k=5):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))\n",
    "    \n",
    "    # Filter: nur Chunks mit mehr als 50 Zeichen UND Frage-Keywords enthalten\n",
    "    keywords = [w.lower() for w in question.split()]\n",
    "    selected_chunks = []\n",
    "    for i in indices[0]:\n",
    "        chunk = chunks[i].strip()\n",
    "        if len(chunk) > 50 and any(kw in chunk.lower() for kw in keywords):\n",
    "            selected_chunks.append(chunk)\n",
    "        if len(selected_chunks) >= k:\n",
    "            break\n",
    "    return \"\\n\".join(selected_chunks)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"Answer the question using ONLY the information from the context.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#TODO entscheide bei jedem max_tokens=1000 ob 1000 angebracht sind\n",
    "def answer_without_context(model, tokenizer, prompt, max_tokens=1000, do_sample=True):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        #-----------------------------------------------------------\n",
    "        enable_thinking=False #-> falls kein thinking model löschen!\n",
    "        #-----------------------------------------------------------\n",
    "    )\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, questions, contexts, max_tokens=1000, do_sample=True):\n",
    "    prompts = []\n",
    "    for q, c in zip(questions, contexts):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {c}\\n\\nQuestion: {q}\"}\n",
    "        ]\n",
    "        full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(full_prompt)\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    generated_answers = []\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    \n",
    "    for i, out_ids in enumerate(output_ids):\n",
    "        new_tokens = out_ids[input_len:]\n",
    "        answer = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        generated_answers.append(answer.strip())\n",
    "        \n",
    "    return generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1206e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_with_context(model, tokenizer, prompt, context, max_tokens=1000, do_sample=True):\n",
    "    rag_instruction = (\n",
    "        \"Answer using the information from the given context.\\n\"\n",
    "        f\"context: {context}\\n\\n\"\n",
    "        f\"{prompt}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": rag_instruction}\n",
    "    ]\n",
    "    \n",
    "    text_input = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True,\n",
    "        #-----------------------------------------------------------\n",
    "        enable_thinking=False #-> falls kein thinking model löschen!\n",
    "        #-----------------------------------------------------------\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_statement_prompt(question: str, answer: str):\n",
    "    return f\"\"\"Given a question and answer, create one or more statements from each sentence in the given answer.\n",
    "question: {question}\n",
    "answer: {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(question: str, answer: str, model, tokenizer, max_tokens=1000):\n",
    "    #Extracts statements out of the model generated answer \n",
    "    prompt = build_statement_prompt(question, answer)\n",
    "\n",
    "    answer = answer_without_context(model, tokenizer, prompt, max_tokens, do_sample=False)\n",
    "\n",
    "    statements = [line.strip() for line in answer.split(\"\\n\") if len(line.strip()) > 3]\n",
    "    return statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71c14b",
   "metadata": {},
   "source": [
    "Changed the prompt for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements):\n",
    "    prompt = f\"\"\"Consider the given context and following statements, the determine whether they are supported by the information presente in the context. \n",
    "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). \n",
    "Answer in the format:\n",
    "Statement: ...\n",
    "Explanation: ...\n",
    "Verdict: (Yes/No)\n",
    "Do not deviate from the specified format.\n",
    "These are the statements\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f\"Statement: {s}\\n\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66adf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_supported(answer: str):\n",
    "    yes_matches = re.findall(r'Verdict:\\s*(Yes)', answer, re.IGNORECASE)\n",
    "    yes_count = len(yes_matches)\n",
    "    return yes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_faithfulness_score(total_statements: int, suported_statements: int):\n",
    "    if suported_statements == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    return suported_statements / total_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_answer_relevance_prompt(answer: str):\n",
    "    prompt = f\"\"\"Generate a question for the given answer.\n",
    "    answer: {answer}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(text: str) -> list:\n",
    "    text.strip()\n",
    "    list = text.split(\"?\")\n",
    "    return [item + \"?\" for item in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_question_similarity(original_question, generated_questions):\n",
    "    q_embedding = embed_model.encode([original_question])\n",
    "    gen_q_embeddings = embed_model.encode(generated_questions)\n",
    "\n",
    "    q_embedding = np.array(q_embedding).astype('float32')\n",
    "    gen_q_embeddings = np.array(gen_q_embeddings).astype('float32')\n",
    "\n",
    "    faiss.normalize_L2(q_embedding)\n",
    "    faiss.normalize_L2(gen_q_embeddings)\n",
    "\n",
    "    d = q_embedding.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    \n",
    "    index.add(gen_q_embeddings)\n",
    "    \n",
    "    k = len(generated_questions)\n",
    "    D, I = index.search(q_embedding, k=k)\n",
    "    \n",
    "    scores = D[0]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def calculate_answer_relevance_score(similatity: list):\n",
    "    return float(np.mean(similatity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_relevance_prompt(question: str):\n",
    "    prompt = f\"\"\"Please extract relevant sentences from the provided context that can potentially help ansewr the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phares \"Insufficient Information\".While extracting candidate sentences you're not allowed to make any changes to senctences from the given context.\n",
    "    Question: {question}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO vielleicht erweitern sehr simple satz zählung -> entscheiden ob auch ! und ? zählen\n",
    "def count_sentences(sentence: str):\n",
    "    num = sentence.count(\". \")\n",
    "    num += sentence.count(\"! \")\n",
    "    num += sentence.count(\"? \")\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_context_relevance_score(num_extracted, num_sentences):\n",
    "    if num_sentences == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return num_extracted / num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_faithfullness(model, tokenizer, context, question, answer, do_sample=True):\n",
    "    #extracts statements out of the answer and generates a statements list\n",
    "    statements = extract_statements(question, answer, model_qwen, tokenizer_qwen)\n",
    "\n",
    "    #lets the model evaluate the faithfullness\n",
    "    faithfullness_prompt = build_faithfullness_prompt(statements)\n",
    "    #do_sample has to be false\n",
    "    eval = answer_with_context(model_qwen, tokenizer_qwen, context, faithfullness_prompt, do_sample=False)\n",
    "    num_supported = count_supported(eval)\n",
    "\n",
    "    return [statements, num_supported]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO entscheiden wieviele fragen die funkition bearbeiten soll\n",
    "def analyse_answer_relevance(model, tokenizer, question, answer, do_sample=True):\n",
    "    prompt = build_answer_relevance_prompt(answer)\n",
    "    questions_generated = answer_without_context(model, tokenizer, prompt, do_sample=do_sample)\n",
    "    questions_generated = extract_questions(questions_generated)\n",
    "\n",
    "    sim = calculate_question_similarity(question, questions_generated)\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_context_relevance(model, tokenizer, context, question):\n",
    "    prompt = build_context_relevance_prompt(question)\n",
    "    answer = answer_with_context(model, tokenizer, prompt, context, do_sample=False)\n",
    "\n",
    "    num_extracted = count_sentences(answer)\n",
    "    num_sentences = count_sentences(context)\n",
    "    \n",
    "    return [num_extracted, num_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_rag(model, tokenizer, dataset, start_index=100, num_samples=5, do_sample=True):\n",
    "    scores = {\n",
    "        \"faithfullness score\" : 0.0,\n",
    "        \"answer relevance score\" : 0.0,\n",
    "        \"context relevance score\" : 0.0\n",
    "    }\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    statements = []\n",
    "    supported_statements = 0\n",
    "    similatity = []\n",
    "    num_extracted = 0\n",
    "    num_sentences = 0\n",
    "\n",
    "    # --- STEP 1: PREPARE DATA BATCH ---\n",
    "    # Extract the list of questions and contexts for the whole batch first\n",
    "    batch_questions = []\n",
    "    batch_contexts = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Access the dataset dynamically based on start_index\n",
    "        item = dataset[start_index + i] \n",
    "        batch_questions.append(item[\"question\"])\n",
    "        batch_contexts.append(item[\"context\"])\n",
    "\n",
    "    # --- STEP 2: BATCH INFERENCE (The Speedup) ---\n",
    "    print(f\"Generating {num_samples} answers in parallel...\")\n",
    "    # We call the model ONCE for all questions. \n",
    "    # Note: Ensure your updated answer_with_context accepts lists for questions/contexts\n",
    "    batch_answers = answer_with_context(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        batch_questions, \n",
    "        batch_contexts, \n",
    "        do_sample=do_sample\n",
    "    )\n",
    "\n",
    "    # --- STEP 3: CALCULATE METRICS (Sequential) ---\n",
    "    # We now iterate through the answers we just generated to calculate scores\n",
    "    print(\"Calculating metrics for generated answers...\")\n",
    "    \n",
    "    for i in tqdm(range(num_samples)):\n",
    "        # Retrieve the specific data for this single iteration\n",
    "        question = batch_questions[i]\n",
    "        context = batch_contexts[i]\n",
    "        answer = batch_answers[i] # Take the answer from our pre-computed list\n",
    "\n",
    "        # 1. Faithfulness\n",
    "        # Note: These functions are still slow if they generate text. \n",
    "        # Ideally, you would batch these too, but this requires rewriting those functions.\n",
    "        faithfullness = analyse_faithfullness(model, tokenizer, context, question, answer, do_sample)\n",
    "        statements.extend(faithfullness[0])\n",
    "        supported_statements += faithfullness[1]\n",
    "\n",
    "        # 2. Answer Relevance\n",
    "        answer_relevance = analyse_answer_relevance(model, tokenizer, question, answer, do_sample=do_sample)\n",
    "        similatity.extend(answer_relevance)\n",
    "\n",
    "        # 3. Context Relevance\n",
    "        context_relevance = analyse_context_relevance(model, tokenizer, context, question)\n",
    "        num_extracted += context_relevance[0]\n",
    "        num_sentences += context_relevance[1]\n",
    "\n",
    "    # --- STEP 4: AGGREGATE SCORES ---\n",
    "    # Prevent division by zero if lists are empty\n",
    "    if len(statements) > 0:\n",
    "        scores[\"faithfullness score\"] = calculate_faithfulness_score(len(statements), supported_statements)\n",
    "    else:\n",
    "        scores[\"faithfullness score\"] = 0\n",
    "        \n",
    "    if len(similatity) > 0:\n",
    "        scores[\"answer relevance score\"] = calculate_answer_relevance_score(similatity)\n",
    "    else:\n",
    "        scores[\"answer relevance score\"] = 0\n",
    "        \n",
    "    scores[\"context relevance score\"] = calculate_context_relevance_score(num_extracted, num_sentences)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774d656",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d18f9f",
   "metadata": {},
   "source": [
    "Calculate scores for Qwen without temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6aca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_rag(model_qwen, tokenizer_qwen, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf195a",
   "metadata": {},
   "source": [
    "Calculate scores for Qwen with temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse_rag(model_qwen, tokenizer_qwen, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
