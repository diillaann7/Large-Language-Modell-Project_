{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128 --quiet\n",
    "!pip install sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas accelerate numpy hf_xet --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\karl\\Large-Language-Modell-Project_\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#------------------------------------------------------------------------------------------------\n",
    "os.environ[\"HF_HOME\"] = \"D:/AI_Models\" #-> only for my computer delete on others!!!\n",
    "#------------------------------------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a378fa",
   "metadata": {},
   "source": [
    "Test if CUDA is available and set the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a993fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU Name: NVIDIA GeForce RTX 5060 Ti\n",
      "Anzahl GPUs: 1\n",
      "CUDA Version (PyTorch): 12.8\n",
      "Accelerator device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"Accelerator device: {accelerator.device}\")\n",
    "else:\n",
    "    accelerator = Accelerator(cpu=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_mistral,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_mistral = accelerator.prepare(model_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_deepseek,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_deepseek = accelerator.prepare(model_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_llama,\n",
    "    dtype=\"auto\"\n",
    "    )\n",
    "model_llama = accelerator.prepare(model_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "#\"Qwen/Qwen2.5-0.5B\" -> \"Qwen/Qwen2-7B\"\n",
    "model_name_qwen = \"Qwen/Qwen2-7B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen) \n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [00:07<00:00, 38.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=accelerator.device)\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=10):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))  \n",
    "    selected_chunks = [chunks[i] for i in indices[0] if len(chunks[i].strip()) > 50]\n",
    "    return \"\\n\".join(selected_chunks[:k])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "Return ONLY complete sentences.\n",
    "Do NOT include explanations, commentary, or unrelated text.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #-> evtl do_sample auf false\n",
    "            temperature=0.01,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=400, k=10):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #-> evtl do_sample auf false\n",
    "            temperature=0.01,    \n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "Faithfulness Score: $$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Consider the given context and following statements, the determine whether they are supported by the information presente in the context. \n",
    "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). \n",
    "Provide a final verdict for each statement in order at the end in the given format. \n",
    "Do not deviate from the specified format.\n",
    "statement: [statement 1]\n",
    "...\n",
    "statement: [statement n]\n",
    "\n",
    "Now evaluate the following statements:\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(question: str, answer: str, model, tokenizer, max_tokens=150):\n",
    "    #Extracts statements out of the model generated answer \n",
    "    \"\"\"\n",
    "    Splits a model-generated answer into complete factual statements.\n",
    "    Each statement will be one line. Partial words or fragments are avoided.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Given a question and answer, create one or more statements from each sentence in the given answer.\n",
    "question: {question}\n",
    "answer: {answer}\"\"\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    statements = [line.strip() for line in output_text.split(\"\\n\") if len(line.strip()) > 3]\n",
    "\n",
    "    return statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "Answer Relevance Score: $$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate up to 3 questions that could be answered by the following answer.\n",
    "Return one question per line.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Frage und Antwort direkt\n",
    "    und skaliert sie auf [0,1].\n",
    "    \"\"\"\n",
    "    if not answer or answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    emb = embed_model.encode([question, answer])\n",
    "    \n",
    "  \n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    \n",
    " \n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "Context Relevance Score: $$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_relevance_prompt():\n",
    "    return f\"\"\"Please extract relevant sentences from the provided context that can potentially help ansewr the following question.\n",
    "If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phares \\\"Insufficient Information\\\".\n",
    "While extracting candidate sentences you're not allowed to make any changes to senctences from the given context.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    " \n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": faithfulness < 1.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    results = []\n",
    "    for i in range(2):  \n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        answer = answer_with_context(model_qwen, tokenizer_qwen, question, max_tokens=50, k=5)\n",
    "\n",
    "        scores = evaluate_rag_answer(\n",
    "            question, answer, context, model_qwen, tokenizer_qwen\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            **scores\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f7135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquestion = \"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"\\n\\n\\ncontext = retrieve_context(question, k=5)\\n\\nprint(\"RETRIEVED CONTEXT:\")\\nprint(context)\\nprint(\"\\n---\\n\")\\n\\n\\n\\nanswer = answer_with_context(\\n    model=model_qwen, \\n    tokenizer=tokenizer_qwen, \\n    question=question,  \\n    max_tokens=200,\\n    k=5\\n)\\n\\nprint(\"ANSWER:\")\\nprint(answer)\\nprint(\"\\n---\\n\")\\n\\n\\nscores = evaluate_rag_answer(\\n    question=question,\\n    answer=answer,\\n    context=context,\\n    model=model_qwen,\\n    tokenizer=tokenizer_qwen\\n)\\n\\nprint(\"EVALUATION SCORES:\")\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "question = \"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"\n",
    "\n",
    "\n",
    "context = retrieve_context(question, k=5)\n",
    "\n",
    "print(\"RETRIEVED CONTEXT:\")\n",
    "print(context)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "answer = answer_with_context(\n",
    "    model=model_qwen, \n",
    "    tokenizer=tokenizer_qwen, \n",
    "    question=question,  \n",
    "    max_tokens=200,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "scores = evaluate_rag_answer(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    context=context,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen\n",
    ")\n",
    "\n",
    "print(\"EVALUATION SCORES:\")\n",
    "print(scores)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
