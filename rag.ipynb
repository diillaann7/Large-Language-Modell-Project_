{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\karl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers datasets transformers jupyterlab_widgets pandas numpy accelerate --quiet\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu121 --quiet\n",
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a378fa",
   "metadata": {},
   "source": [
    "Test if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a993fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU Name: NVIDIA GeForce RTX 2060\n",
      "Anzahl GPUs: 1\n",
      "CUDA Version (PyTorch): 12.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Anzahl GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version (PyTorch): {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "041546c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962a597ba7e64d36be2ad469bd263d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc971832c264430ae3b58cfdc3fe8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2983d85e5040469e96a46677bceeb50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb04a904a4845c7aba19cb42a65f68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d59d135ec14e389ab22bfc830b8c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ff6ecf88cf450882b366ae927d7c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa15bfdeaf42b8a37966b68da0b38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cbce0f5d60411facb03da745297b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf66d1e3bdd42efae842638e8c6bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba89e1fd7f7445e952ff11288ab3a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241162c3884549dfbd860cc58b0b3dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(model_name_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "accelerator = Accelerator()\n",
    "#\"Qwen/Qwen2.5-0.5B\" -> \"Qwen/Qwen2-7B\"\n",
    "model_name_qwen = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_qwen,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "model_qwen = accelerator.prepare(model_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119ebd7ad4f5494a80b98669f38d634c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=10):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))  \n",
    "    selected_chunks = [chunks[i] for i in indices[0] if len(chunks[i].strip()) > 50]\n",
    "    return \"\\n\".join(selected_chunks[:k])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "Return ONLY complete sentences.\n",
    "Do NOT include explanations, commentary, or unrelated text.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.01,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Slicing to get only the new tokens\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=400, k=10):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\") -> falls keine CUDA fähige grafikkarte verwendet wird entkommentieren\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True, #-> evtl do_sample auf false\n",
    "            temperature=0.01,    \n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "Faithfulness Score: $$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Consider the given context and following statements, the determine whether they are supported by the information presente in the context.\n",
    "Provide a brief explanation for each statement before arriving at the verdict (Yes/No). \n",
    "Provide a final verdict for each statement in order at the end in the given format. \n",
    "Do not deviate from the specified format.\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(answer: str, model, tokenizer, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Splits a model-generated answer into complete factual statements.\n",
    "    Each statement will be one line. Partial words or fragments are avoided.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Split the following answer into complete factual statements.\n",
    "Return one complete sentence per line.\n",
    "Do NOT add explanations, bullet points, or partial words.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    statements = [line.strip() for line in output_text.split(\"\\n\") if len(line.strip()) > 3]\n",
    "\n",
    "    return statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "Answer Relevance Score: $$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate a question for the given answer.\n",
    "Answer:{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "Context relevance score: $$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Frage und Antwort direkt\n",
    "    und skaliert sie auf [0,1].\n",
    "    \"\"\"\n",
    "    if not answer or answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    emb = embed_model.encode([question, answer])\n",
    "    \n",
    "  \n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    \n",
    " \n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    " \n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": faithfulness < 1.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    results = []\n",
    "    for i in range(2):  \n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        answer = answer_with_context(model_qwen, tokenizer_qwen, question, max_tokens=50, k=5)\n",
    "\n",
    "        scores = evaluate_rag_answer(\n",
    "            question, answer, context, model_qwen, tokenizer_qwen\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            **scores\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquestion = \"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"\\n\\n\\ncontext = retrieve_context(question, k=5)\\n\\nprint(\"RETRIEVED CONTEXT:\")\\nprint(context)\\nprint(\"\\n---\\n\")\\n\\n\\n\\nanswer = answer_with_context(\\n    model=model_qwen, \\n    tokenizer=tokenizer_qwen, \\n    question=question,  \\n    max_tokens=200,\\n    k=5\\n)\\n\\nprint(\"ANSWER:\")\\nprint(answer)\\nprint(\"\\n---\\n\")\\n\\n\\nscores = evaluate_rag_answer(\\n    question=question,\\n    answer=answer,\\n    context=context,\\n    model=model_qwen,\\n    tokenizer=tokenizer_qwen\\n)\\n\\nprint(\"EVALUATION SCORES:\")\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "question = \"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"\n",
    "\n",
    "\n",
    "context = retrieve_context(question, k=5)\n",
    "\n",
    "print(\"RETRIEVED CONTEXT:\")\n",
    "print(context)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "answer = answer_with_context(\n",
    "    model=model_qwen, \n",
    "    tokenizer=tokenizer_qwen, \n",
    "    question=question,  \n",
    "    max_tokens=200,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "scores = evaluate_rag_answer(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    context=context,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen\n",
    ")\n",
    "\n",
    "print(\"EVALUATION SCORES:\")\n",
    "print(scores)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
