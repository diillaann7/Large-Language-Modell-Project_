{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers torch datasets transformers jupyterlab_widgets pandas numpy --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dilan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(model_name_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_name_qwen = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(model_name_qwen)\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [02:26<00:00,  1.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=10):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))  \n",
    "    selected_chunks = [chunks[i] for i in indices[0] if len(chunks[i].strip()) > 50]\n",
    "    return \"\\n\".join(selected_chunks[:k])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=800):\n",
    "    tokanized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokanized_input[\"input_ids\"]\n",
    "\n",
    "    prompt_length = input_ids.shape[1]\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids).logits\n",
    "        output = output.squeeze(dim=0)\n",
    "        next_token_scores = output[-1]\n",
    "        next_token_id = next_token_scores.argmax(dim=-1)\n",
    "        input_ids = torch.cat((input_ids, torch.LongTensor([next_token_id]).reshape(1, -1)), dim=-1)\n",
    "\n",
    "    \n",
    "    generated_ids = input_ids[0, prompt_length:]\n",
    "\n",
    "    return tokenizer.decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=400, k=10):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,       # sampling aktivieren\n",
    "            temperature=0.7,     # kreative Antworten\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Split the following answer into simple factual statements.\n",
    "Return one statement per line. Do not add anything else.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [s.strip(\"- \").strip() for s in output.split(\"\\n\") if len(s.strip()) > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate up to 3 questions that could be answered by the following answer.\n",
    "Return one question per line.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(question: str, answer: str):\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Frage und Antwort direkt\n",
    "    und skaliert sie auf [0,1].\n",
    "    \"\"\"\n",
    "    if not answer or answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    emb = embed_model.encode([question, answer])\n",
    "    \n",
    "  \n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    \n",
    " \n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    " \n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": faithfulness < 1.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "For each of the statements below, decide if it is fully supported by the context.\n",
    "Respond with ONLY 'Yes' or 'No'.\n",
    "Do not add any explanations.\n",
    "\n",
    "Examples:\n",
    "Statement: \"Grasse Cathedral is the town's most notable landmark.\"\n",
    "Answer: Yes\n",
    "\n",
    "Statement: \"Grasse is the capital of France.\"\n",
    "Answer: No\n",
    "\n",
    "Now evaluate the following statements:\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse():\n",
    "    results = []\n",
    "    for i in range(2):  \n",
    "        question = rag_dataset_1200[i][\"question\"]\n",
    "        context = rag_dataset_1200[i][\"context\"]\n",
    "        answer = answer_with_context(model_qwen, tokenizer_qwen, question, max_tokens=50, k=5)\n",
    "\n",
    "        scores = evaluate_rag_answer(\n",
    "            question, answer, context, model_qwen, tokenizer_qwen\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            **scores\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVED CONTEXT:\n",
      "Francisco Rogers found the answer to a search query collar george herbert essay\n",
      "Link ----> collar george herbert essay\n",
      "Write my essay ESSAYERUDITE.COM\n",
      "constitution research paper ideas\n",
      "definition essay humility\n",
      "business strategy case study solution\n",
      "corporals course essay\n",
      "decisions in paradise essays\n",
      "college essay word count\n",
      "credit cart terminal paper\n",
      "byron don juan essay\n",
      "democratic party essays\n",
      "coursework language learning material teaching\n",
      "christmas commercialized essay\n",
      "dahrendorf essays theory\n",
      " golconda fort the history of the maghrib an interpretive essay essay of the story of an hour best essays for high school students classical argument essay format essay question writing ncea level 2 english essay structure essays on the delian league cause and effect essay rubrics deloitte case studies essay on fashion among youth fine art dissertation examples thesis about arthritis buy term paper now why i want to be a cop essay referencing chapters in an essay thesis statement in a essay essa\n",
      "I took my final for the mini session today, after three VERY solid days of studying. Not sure how I did because he kind of caught everyone off guard with some of the questions but pretty much right now I am The Complete Civil War Encyclopedia: Callison Edition. And oh, is it the most random information. Percentage of Civil War deaths bayonet-related? 2. Name of Ulysses S. Grant’s horse? Cincinnati. Name of the guy who invented the “land torpedo” that would evolve into the modern land mine? Peter\n",
      " essays on british history finishing the dissertation roman social classes essay essay on the age of innocence sample cover sheets for research papers essay questions for higher biology model history essay technique de dissertation juridique sophocles vs euripides essay thesis example elementary education project based thesis ann mcclintock essay essay on the best way of spending holidays essay on golconda fort the history of the maghrib an interpretive essay essay of the story of an hour best e\n",
      " most of the stuff in MG is about textual searches > rather than exact > > searches > > Yeah, but that's the most difficult thing ;-) > > > (it can do boolean searches too, but the book is > mainly about > > ranking). > > Please god tell me they cover phrase matching :-S > > [snip headf*ck] > > Urm, maybe they'll take it a little slower than > that? ;-) > > cheers, > > Chris > > __________________________________________________ Do You Yahoo!? Buy the perfect holiday gifts at Yahoo! Shopping. __\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      "I don't know.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION SCORES:\n",
      "{'faithfulness': 0.1, 'answer_relevance': np.float32(0.5739031), 'hallucination': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Who found the answer to a search query collar george herbert essay?\"\n",
    "\n",
    "\n",
    "context = retrieve_context(question, k=5)\n",
    "\n",
    "print(\"RETRIEVED CONTEXT:\")\n",
    "print(context)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "answer = answer_with_context(\n",
    "    model=model_qwen, \n",
    "    tokenizer=tokenizer_qwen, \n",
    "    question=question,  \n",
    "    max_tokens=200,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\")\n",
    "print(answer)\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "scores = evaluate_rag_answer(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    context=context,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen\n",
    ")\n",
    "\n",
    "print(\"EVALUATION SCORES:\")\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
