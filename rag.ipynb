{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e2b871",
   "metadata": {},
   "source": [
    "# Comparing The Effectiveness Of RAG Between Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78698",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu transformers torch datasets transformers jupyterlab_widgets pandas numpy --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b27af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dilan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import faiss\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d73e4",
   "metadata": {},
   "source": [
    "### Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_name_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
    "\n",
    "model_name_deepseek = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "model_deepseek = AutoModelForCausalLM.from_pretrained(model_name_deepseek)\n",
    "tokenizer_deepseek = AutoTokenizer.from_pretrained(model_name_deepseek)\n",
    "\n",
    "model_name_llama = \"meta-llama/Llama-2-7b\"\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama)\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_name_qwen = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(model_name_qwen)\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name_qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4cbb",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7c8fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trivia_qa = datasets.load_dataset(\"mandarjoshi/trivia_qa\", \"rc\", split=\"train\")\n",
    "\n",
    "documents_trivia_qa = [item[\"search_results\"][\"context\"][0] for item in trivia_qa if item[\"search_results\"][\"context\"]]\n",
    "len(documents_trivia_qa)\n",
    "\"\"\"\n",
    "\n",
    "rag_dataset_1200 = datasets.load_dataset(\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
    "\n",
    "documents_rag_1200 = [item[\"context\"] for item in rag_dataset_1200]\n",
    "len(documents_rag_1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e1101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents_rag_1200\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6642c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 273/273 [02:28<00:00,  1.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8732, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "embeddings = embed_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd240f",
   "metadata": {},
   "source": [
    "We use the FlatL2 index to measure the similarity of the embeddings. FlatL2 ueses the euclidian distance for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b20b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8732\n",
      "(8732, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "print(len(embeddings))  \n",
    "if len(embeddings) > 0:\n",
    "    print(np.array(embeddings).shape)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98fa829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=10):\n",
    "    q_emb = embed_model.encode([question])\n",
    "    distances, indices = index.search(q_emb, len(chunks))  \n",
    "    selected_chunks = [chunks[i] for i in indices[0] if len(chunks[i].strip()) > 50]\n",
    "    return \"\\n\".join(selected_chunks[:k])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205afb7",
   "metadata": {},
   "source": [
    "Prompt similar the the one from the Ragas paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b7b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "Answer the question using ONLY the information from the context.\n",
    "Return ONLY complete sentences.\n",
    "Do NOT include explanations, commentary, or unrelated text.\n",
    "If the answer is not explicitly stated, reply with: I don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf7d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_without_context(model, tokenizer, prompt, max_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c000a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(model, tokenizer, question, max_tokens=400, k=10):\n",
    "    context = retrieve_context(question, k=k)\n",
    "    prompt = build_rag_prompt(context, question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,       \n",
    "            temperature=0.7,    \n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated = output_ids[0][prompt_len:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e69f2",
   "metadata": {},
   "source": [
    "### The three scores mentioned in the Ragas paper\n",
    "In this section we implement all three scores mentioned in the Ragas paper.\n",
    "We will use these to evaluate the models rag performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18760415",
   "metadata": {},
   "source": [
    "$$F = \\frac{|F|}{|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdd0e1",
   "metadata": {},
   "source": [
    "$$AR = \\frac{1}{n} \\sum\\limits^n_{i=1} sim(q,q_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa70d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_similarity(q: str, q_answer: str) -> float:\n",
    "    emb = embed_model.encode([q, q_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (norm(emb[0]) * norm(emb[1]))\n",
    "    return (sim + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77997f7",
   "metadata": {},
   "source": [
    "$$CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of senctences in }c(q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b957b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements(answer: str, model, tokenizer, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Splits a model-generated answer into complete factual statements.\n",
    "    Each statement will be one line. Partial words or fragments are avoided.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Split the following answer into complete factual statements.\n",
    "Return one complete sentence per line.\n",
    "Do NOT add explanations, bullet points, or partial words.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    statements = [line.strip() for line in output_text.split(\"\\n\") if len(line.strip()) > 3]\n",
    "\n",
    "    return statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(statements, context, model, tokenizer):\n",
    "    if len(statements) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    prompt = build_faithfullness_prompt(statements, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,      \n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    answer_lines = [line.split(\"Answer:\")[-1].strip() for line in output.split(\"\\n\") if \"Answer:\" in line]\n",
    "\n",
    "    supported = sum(1 for line in answer_lines if line.lower().startswith(\"yes\"))\n",
    "\n",
    "    return supported / len(statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "957d4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_answer(answer: str, model, tokenizer, max_tokens=150):\n",
    "    prompt = f\"\"\"\n",
    "Generate up to 3 questions that could be answered by the following answer.\n",
    "Return one question per line.\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return [q.strip() for q in output.split(\"\\n\") if q.strip().endswith(\"?\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a13e600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness_given_answer(question, context, answer, model, tokenizer, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Prüft, ob die gegebene Antwort vollständig durch den Context gestützt wird.\n",
    "    Rückgabe: Float zwischen 0 und 1 (1 = vollständig unterstützt)\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an evaluator. Given a context and a candidate answer to a question,\n",
    "assess whether the answer is fully supported by the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Is the answer fully supported by the context? Answer only 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip().lower()\n",
    "\n",
    "    # Konvertiere Yes/No in 1/0\n",
    "    if \"yes\" in output_text:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_score(question: str, generated_questions: list):\n",
    "    if len(generated_questions) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0\n",
    "    for q in generated_questions:\n",
    "        score += eval_similarity(question, q)\n",
    "\n",
    "    return score / len(generated_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b35659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_answer_relevance_direct(ground_truth: str, llm_answer: str, embed_model=None) -> float:\n",
    "    \"\"\"\n",
    "    Misst die semantische Ähnlichkeit zwischen Ground-Truth und LLM-Antwort (0-1).\n",
    "    Rückgabe: 1 = sehr ähnlich, 0 = keine Ähnlichkeit\n",
    "    \"\"\"\n",
    "    if not llm_answer or llm_answer.lower() in [\"i don't know\", \"unknown\"]:\n",
    "        return 0.0\n",
    "    \n",
    "    emb = embed_model.encode([ground_truth, llm_answer])\n",
    "    sim = np.dot(emb[0], emb[1]) / (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1]))\n",
    "    return (sim + 1) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6005fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucination(faithfulness: float, answer_relevance: float, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"\n",
    "    Prüft, ob die Antwort halluziniert ist.\n",
    "    True = Halluzination, False = keine Halluzination\n",
    "    \"\"\"\n",
    "    return faithfulness < threshold or answer_relevance < threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ef4b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer_given_answer(ground_truth: str, llm_answer: str, embed_model=None, threshold: float = 0.5) -> dict:\n",
    "    \"\"\"\n",
    "    Kombiniert alle Scores für eine einzelne LLM-Antwort.\n",
    "    Rückgabe: dict mit 'faithfulness', 'answer_relevance', 'hallucination'\n",
    "    \"\"\"\n",
    "    answer_relevance = calculate_answer_relevance_direct(ground_truth, llm_answer, embed_model=embed_model)\n",
    "    faithfulness = evaluate_faithfulness_vs_groundtruth(ground_truth, llm_answer, embed_model=embed_model)\n",
    "    hallucination = detect_hallucination(faithfulness, answer_relevance, threshold=threshold)\n",
    "    \n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": hallucination\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "051db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness_vs_groundtruth(ground_truth: str, llm_answer: str, embed_model=None) -> float:\n",
    "    \"\"\"\n",
    "    Prüft, wie stark die LLM-Antwort durch die Ground-Truth gestützt wird.\n",
    "    Hier gleichgesetzt mit der semantischen Ähnlichkeit.\n",
    "    \"\"\"\n",
    "    return calculate_answer_relevance_direct(ground_truth, llm_answer, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e98133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer(question, answer, context, model, tokenizer):\n",
    "    if answer.lower() in [\"i don't know\", \"\"]:\n",
    "        return {\n",
    "            \"faithfulness\": 0.0,\n",
    "            \"answer_relevance\": 0.0,\n",
    "            \"hallucination\": True\n",
    "        }\n",
    "\n",
    "    # Anpassung: kurze Antworten nicht splitten\n",
    "    if len(answer.strip()) < 100:\n",
    "        statements = [answer.strip()]\n",
    "    else:\n",
    "        statements = extract_statements(answer, model, tokenizer)[:10]\n",
    "\n",
    "    if not statements:\n",
    "        faithfulness = 0.0\n",
    "    else:\n",
    "        faithfulness = evaluate_faithfulness(statements, context, model, tokenizer)\n",
    "\n",
    "    answer_relevance = calculate_answer_relevance_direct(question, answer)\n",
    "\n",
    "    # Hallucination nur, wenn keine Aussage gestützt wird\n",
    "    hallucination = faithfulness == 0.0\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\":bool((faithfulness < 0.5) or (answer_relevance < 0.5))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f653f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faithfullness_prompt(statements, context):\n",
    "    prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "For each of the statements below, decide if it is fully supported by the context.\n",
    "Respond with ONLY 'Yes' or 'No'.\n",
    "Do not add any explanations.\n",
    "\n",
    "Examples:\n",
    "Statement: \"Grasse Cathedral is the town's most notable landmark.\"\n",
    "Answer: Yes\n",
    "\n",
    "Statement: \"Grasse is the capital of France.\"\n",
    "Answer: No\n",
    "\n",
    "Now evaluate the following statements:\n",
    "\"\"\"\n",
    "    for s in statements:\n",
    "        prompt += f'Statement: \"{s}\"\\nAnswer: '\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b5f4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_answer_vs_groundtruth(question, llm_answer, ground_truth, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Vergleicht die LLM-Antwort mit der Ground-Truth-Antwort.\n",
    "    \n",
    "    Args:\n",
    "        question: str, die Frage\n",
    "        llm_answer: str, vom Modell generierte Antwort\n",
    "        ground_truth: str, richtige Antwort aus dem Dataset\n",
    "        threshold: float, Schwellenwert für semantische Ähnlichkeit\n",
    "    \n",
    "    Returns:\n",
    "        dict mit faithfulness, answer_relevance, hallucination\n",
    "    \"\"\"\n",
    "    # 1. Answer Relevance: semantische Ähnlichkeit zwischen LLM-Antwort und Ground-Truth\n",
    "    answer_relevance = calculate_answer_relevance_direct(ground_truth, llm_answer)\n",
    "\n",
    "    # 2. Faithfulness: wie stark die LLM-Antwort durch die Ground-Truth gestützt wird\n",
    "    # = 1 wenn sehr ähnlich, sonst 0\n",
    "    faithfulness = answer_relevance\n",
    "\n",
    "    # 3. Hallucination: True wenn die Antwort nicht der Ground-Truth ähnelt\n",
    "    hallucination = answer_relevance < threshold\n",
    "\n",
    "    return {\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevance\": answer_relevance,\n",
    "        \"hallucination\": hallucination\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b851f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_with_groundtruth(questions, ground_truths, model, tokenizer, embed_model, k=5, max_tokens=200, threshold=0.5):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for question, gt_answer in zip(questions, ground_truths):\n",
    "        # 1. Kontext abrufen (optional, für RAG)\n",
    "        context = retrieve_context(question, k=k)\n",
    "\n",
    "        # 2. LLM generiert Antwort unter Verwendung des Kontextes\n",
    "        llm_answer = answer_with_context(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            question=question,\n",
    "            max_tokens=max_tokens,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        # 3. Scores berechnen im Vergleich zur Ground-Truth\n",
    "        scores = evaluate_rag_answer_vs_groundtruth(\n",
    "            question=question,\n",
    "            llm_answer=llm_answer,\n",
    "            ground_truth=gt_answer,\n",
    "            threshold=threshold\n",
    "        )\n",
    "\n",
    "        # 4. Ergebnisse speichern\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"ground_truth\": gt_answer,\n",
    "            \"llm_answer\": llm_answer,\n",
    "            **scores\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdfd8",
   "metadata": {},
   "source": [
    "### RAG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c123162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_with_model(questions, model, tokenizer, k=5, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Für jede Frage:\n",
    "    1. Kontext aus Dokumenten abrufen\n",
    "    2. LLM generiert eine Antwort\n",
    "    3. Faithfulness, Answer Relevance und Hallucination berechnen\n",
    "    4. Ergebnisse zurückgeben\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for question in questions:\n",
    "        # 1. Kontext abrufen\n",
    "        context = retrieve_context(question, k=k)\n",
    "\n",
    "        # 2. Antwort vom Modell generieren\n",
    "        answer = answer_with_context(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            question=question,\n",
    "            max_tokens=max_tokens,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        # 3. Scores berechnen\n",
    "        scores = evaluate_rag_answer_given_answer(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context=context,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        # 4. Ergebnisse speichern\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"answer\": answer,\n",
    "            **scores\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a34e4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m questions = [\u001b[33m\"\u001b[39m\u001b[33mWho were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m ground_truths = [\u001b[33m\"\u001b[39m\u001b[33mRyan O’Reilly, Brian Gionta, and Leon Draisaitl were the three stars.\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43manalyse_with_groundtruth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mground_truths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mground_truths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_qwen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_qwen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- wichtig!\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m, res[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36manalyse_with_groundtruth\u001b[39m\u001b[34m(questions, ground_truths, model, tokenizer, embed_model, k, max_tokens, threshold)\u001b[39m\n\u001b[32m     10\u001b[39m llm_answer = answer_with_context(\n\u001b[32m     11\u001b[39m     model=model,\n\u001b[32m     12\u001b[39m     tokenizer=tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     k=k\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 3. Scores berechnen im Vergleich zur Ground-Truth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m scores = \u001b[43mevaluate_rag_answer_vs_groundtruth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgt_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 4. Ergebnisse speichern\u001b[39;00m\n\u001b[32m     27\u001b[39m results.append({\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: context,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     **scores\n\u001b[32m     33\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mevaluate_rag_answer_vs_groundtruth\u001b[39m\u001b[34m(question, llm_answer, ground_truth, threshold)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mVergleicht die LLM-Antwort mit der Ground-Truth-Antwort.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33;03m    dict mit faithfulness, answer_relevance, hallucination\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1. Answer Relevance: semantische Ähnlichkeit zwischen LLM-Antwort und Ground-Truth\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m answer_relevance = \u001b[43mcalculate_answer_relevance_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2. Faithfulness: wie stark die LLM-Antwort durch die Ground-Truth gestützt wird\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# = 1 wenn sehr ähnlich, sonst 0\u001b[39;00m\n\u001b[32m     19\u001b[39m faithfulness = answer_relevance\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalculate_answer_relevance_direct\u001b[39m\u001b[34m(ground_truth, llm_answer, embed_model)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llm_answer \u001b[38;5;129;01mor\u001b[39;00m llm_answer.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mi don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m emb = \u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m([ground_truth, llm_answer])\n\u001b[32m     10\u001b[39m sim = np.dot(emb[\u001b[32m0\u001b[39m], emb[\u001b[32m1\u001b[39m]) / (np.linalg.norm(emb[\u001b[32m0\u001b[39m]) * np.linalg.norm(emb[\u001b[32m1\u001b[39m]))\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (sim + \u001b[32m1\u001b[39m) / \u001b[32m2\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "questions = [\"Who were the three stars in the NHL game between Buffalo Sabres and Edmonton Oilers?\"]\n",
    "ground_truths = [\"Ryan O’Reilly, Brian Gionta, and Leon Draisaitl were the three stars.\"]\n",
    "\n",
    "results = analyse_with_groundtruth(\n",
    "    questions=questions,\n",
    "    ground_truths=ground_truths,\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    embed_model=embed_model,  # <- wichtig!\n",
    "    k=5,\n",
    "    max_tokens=200,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(\"Question:\", res[\"question\"])\n",
    "    print(\"LLM Answer:\", res[\"llm_answer\"])\n",
    "    print(\"Faithfulness:\", res[\"faithfulness\"])\n",
    "    print(\"Answer Relevance:\", res[\"answer_relevance\"])\n",
    "    print(\"Hallucination:\", res[\"hallucination\"])\n",
    "    print(\"\\n---\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
